{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995f6dd8",
   "metadata": {},
   "source": [
    "### Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49d00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1f05a",
   "metadata": {},
   "source": [
    "# Metryki używane do oceny modeli klasyfikacyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d452aa",
   "metadata": {},
   "source": [
    "## Metryki - Klasyfikacja binarna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f8c3e",
   "metadata": {},
   "source": [
    "### Macierz błędów (Confusion Matrix)\n",
    "\n",
    "Macierz błędów to tabela, która przedstawia wyniki przewidywań modelu w porównaniu do rzeczywistych klas. W przypadku klasyfikacji binarnej macierz ma postać 2x2 i zawiera cztery główne komponenty:\n",
    "\n",
    "- **True Positives (TP)**: Liczba poprawnie sklasyfikowanych pozytywnych przypadków.\n",
    "- **True Negatives (TN)**: Liczba poprawnie sklasyfikowanych negatywnych przypadków.\n",
    "- **False Positives (FP)**: Liczba negatywnych przypadków, które zostały błędnie sklasyfikowane jako pozytywne.\n",
    "- **False Negatives (FN)**: Liczba pozytywnych przypadków, które zostały błędnie sklasyfikowane jako negatywne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42c3f7",
   "metadata": {},
   "source": [
    "![Macierz_pomylek](mp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b6cdf",
   "metadata": {},
   "source": [
    "### Dokładność (Accuracy)\n",
    "\n",
    "Dokładność to jedna z najprostszych miar oceny wydajności modelu. Oblicza się ją jako stosunek liczby poprawnych przewidywań do całkowitej liczby próbek:\n",
    "\n",
    "####   $$Accuracy = \\frac{correct\\ predictions}{total\\ predictions} * 100$$\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Dokładność jest przydatna w przypadku zrównoważonych zbiorów danych, ale może być myląca w przypadku niezrównoważonych klas, gdzie jedna klasa dominuje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e091ff97",
   "metadata": {},
   "source": [
    "### Precyzja (Precision)\n",
    "\n",
    "Precyzja mierzy dokładność pozytywnych przewidywań modelu. Jest to stosunek liczby prawdziwie pozytywnych do wszystkich przewidywanych pozytywnych:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Wysoka precyzja oznacza, że model rzadko klasyfikuje negatywne przypadki jako pozytywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e049a11",
   "metadata": {},
   "source": [
    "### Czułość (Recall)\n",
    "\n",
    "Czułość, znana również jako współczynnik prawdziwie pozytywny (True Positive Rate), ocenia, ile rzeczywistych pozytywnych przypadków zostało poprawnie wykrytych przez model:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Wysoka czułość oznacza, że model skutecznie identyfikuje pozytywne przypadki, minimalizując błędy typu II (False Negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde8a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e93aa89",
   "metadata": {},
   "source": [
    "### Wskaźnik F1 (F1 Score)\n",
    "\n",
    "Wskaźnik F1 to średnia harmoniczna precyzji i czułości, która łączy obie metryki w jedną wartość. Jest szczególnie przydatny w sytuacjach, gdy istnieje potrzeba zrównoważenia między precyzją a czułością:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Wskaźnik F1 jest użyteczny, gdy klasy są niezrównoważone i ważne jest, aby zarówno precyzja, jak i czułość były na wysokim poziomie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd41728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7e2926",
   "metadata": {},
   "source": [
    "### Raport klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06e55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92d3984c",
   "metadata": {},
   "source": [
    "## Krzywa ROC i AUC\n",
    "\n",
    "Krzywa ROC (Receiver Operating Characteristic) to wykres, który przedstawia zależność między czułością (True Positive Rate) a współczynnikiem fałszywie pozytywnym (False Positive Rate) dla różnych progów decyzyjnych modelu.\n",
    "\n",
    "AUC (Area Under the Curve) to pole pod krzywą ROC, które mierzy zdolność modelu do rozróżniania klas. AUC przyjmuje wartości od 0 do 1:\n",
    "\n",
    "- AUC = 1 oznacza idealny model, który doskonale rozróżnia klasy.\n",
    "- AUC = 0,5 oznacza model losowy, który nie ma zdolności do klasyfikacji.\n",
    "- AUC < 0,5 oznacza model gorzej niż losowy.\n",
    "\n",
    "Przykład interpretacji: Model ma AUC równe 0,85, co oznacza, że model ma dobrą zdolność do rozróżniania klas. Innymi słowy, prawdopodobieństwo, że model przypisze wyższą wartość prawdopodobieństwa do pozytywnej obserwacji niż do negatywnej, wynosi 0,85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1ffce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca13b5a2",
   "metadata": {},
   "source": [
    "## Metryki - Klasyfikacja wieloklasowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e546dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
