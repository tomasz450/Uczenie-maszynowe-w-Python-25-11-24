{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af93b89",
   "metadata": {},
   "source": [
    "## Wyzwania i ograniczenia uczenia maszynowego\n",
    "\n",
    "### Jakość i ilość danych\n",
    "\n",
    "a) Znaczenie danych w uczeniu maszynowym\n",
    "   - Dane jako fundament modeli ML\n",
    "   - Zasada \"garbage in, garbage out\" w kontekście ML\n",
    "\n",
    "b) Problemy związane z ilością danych\n",
    "   - Niewystarczająca ilość danych\n",
    "     - Trudności w trenowaniu złożonych modeli\n",
    "     - Ryzyko przeuczenia (overfitting)\n",
    "   - Nadmiar danych\n",
    "     - Wyzwania związane z przetwarzaniem i przechowywaniem\n",
    "     - Konieczność efektywnej filtracji i selekcji istotnych informacji\n",
    "\n",
    "c) Wyzwania dotyczące jakości danych\n",
    "   - Niekompletne dane\n",
    "     - Brakujące wartości i ich wpływ na modele\n",
    "     - Techniki radzenia sobie z brakującymi danymi (imputacja, usuwanie)\n",
    "   - Niezbilansowane zbiory danych\n",
    "     - Problemy w klasyfikacji z nierównomiernie reprezentowanymi klasami\n",
    "     - Techniki balansowania zbiorów (oversampling, undersampling, SMOTE)\n",
    "   - Zaszumione dane\n",
    "     - Wpływ błędów pomiarowych i losowych na jakość modeli\n",
    "     - Metody redukcji szumu w danych\n",
    "\n",
    "d) Problemy z reprezentatywnością danych\n",
    "   - Stronniczość w zbiorach danych\n",
    "     - Źródła stronniczości (historyczna, demograficzna, metodologiczna)\n",
    "     - Konsekwencje trenowania modeli na stronniczych danych\n",
    "   - Zmiana rozkładu danych w czasie (concept drift)\n",
    "     - Wyzwania związane z utrzymaniem aktualności modeli\n",
    "     - Techniki adaptacyjnego uczenia się\n",
    "\n",
    "e) Prywatność i bezpieczeństwo danych\n",
    "   - Regulacje prawne (np. GDPR, CCPA)\n",
    "   - Techniki uczenia federacyjnego i rozproszonego\n",
    "   - Uczenie maszynowe z zachowaniem prywatności (differential privacy)\n",
    "\n",
    "7.2. Obliczeniowa złożoność\n",
    "\n",
    "a) Rosnące wymagania obliczeniowe\n",
    "   - Trenowanie dużych modeli (np. transformery w NLP)\n",
    "   - Skalowanie infrastruktury do obsługi big data\n",
    "\n",
    "b) Czas trenowania modeli\n",
    "   - Kompromis między czasem trenowania a jakością modelu\n",
    "   - Techniki przyspieszonego treningu (transfer learning, fine-tuning)\n",
    "\n",
    "c) Złożoność algorytmów uczenia maszynowego\n",
    "   - Analiza asymptotyczna złożoności czasowej i pamięciowej\n",
    "   - Wyzwania związane z algorytmami o wysokiej złożoności (np. SVM z nieliniowymi jądrami)\n",
    "\n",
    "d) Optymalizacja obliczeń\n",
    "   - Wykorzystanie GPU i TPU do przyspieszenia obliczeń\n",
    "   - Techniki redukcji wymiarowości (PCA, t-SNE)\n",
    "   - Kwantyzacja modeli i obliczenia o obniżonej precyzji\n",
    "\n",
    "e) Energochłonność i wpływ na środowisko\n",
    "   - Carbon footprint dużych modeli ML\n",
    "   - Zielone uczenie maszynowe i techniki redukcji zużycia energii\n",
    "\n",
    "f) Wyzwania związane z wdrażaniem modeli\n",
    "   - Optymalizacja modeli do działania na urządzeniach brzegowych (edge computing)\n",
    "   - Techniki kompresji modeli dla aplikacji mobilnych\n",
    "\n",
    "7.3. Generalizacja modeli\n",
    "\n",
    "a) Koncepcja generalizacji w uczeniu maszynowym\n",
    "   - Definicja i znaczenie generalizacji\n",
    "   - Różnica między dopasowaniem do danych treningowych a rzeczywistą wydajnością\n",
    "\n",
    "b) Przeuczenie (overfitting) i niedouczenie (underfitting)\n",
    "   - Przyczyny i konsekwencje przeuczenia\n",
    "   - Techniki zapobiegania przeuczeniu (regularyzacja, dropout, early stopping)\n",
    "   - Rozpoznawanie i radzenie sobie z niedouczeniem\n",
    "\n",
    "c) Kompromis między obciążeniem a wariancją (bias-variance tradeoff)\n",
    "   - Zrozumienie roli obciążenia i wariancji w błędzie generalizacji\n",
    "   - Techniki balansowania obciążenia i wariancji\n",
    "\n",
    "d) Walidacja krzyżowa i techniki oceny modeli\n",
    "   - Znaczenie prawidłowej walidacji modeli\n",
    "   - Różne schematy walidacji krzyżowej (k-fold, leave-one-out)\n",
    "   - Testowanie na niezależnych zbiorach danych\n",
    "\n",
    "e) Przenośność modeli\n",
    "   - Wyzwania związane z przenoszeniem modeli między domenami\n",
    "   - Techniki adaptacji domenowej i transfer learning\n",
    "\n",
    "f) Interpretacja i wyjaśnialność modeli\n",
    "   - Znaczenie zrozumienia decyzji podejmowanych przez modele\n",
    "   - Techniki interpretacji modeli (SHAP, LIME, attention mechanisms)\n",
    "   - Kompromis między wydajnością a interpretowalnością\n",
    "\n",
    "g) Odporność modeli na ataki i manipulacje\n",
    "   - Przykłady przeciwne (adversarial examples)\n",
    "   - Techniki zwiększania odporności modeli na ataki\n",
    "\n",
    "h) Uczenie ciągłe i adaptacja do zmian\n",
    "   - Wyzwania związane z utrzymaniem aktualności modeli w zmieniającym się środowisku\n",
    "   - Techniki uczenia przyrostowego i online learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
